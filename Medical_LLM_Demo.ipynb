{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzJBGR6_qdql"
   },
   "outputs": [],
   "source": [
    "# üè• MEDICAL LLM LIVE DEMO\n",
    "# This notebook loads the fine-tuned ChatDoctor model directly from Hugging Face.\n",
    "\n",
    "# 1. Install dependencies (Quiet mode)\n",
    "print(\"‚è≥ Installing libraries... (approx. 2 mins)\")\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes gradio -q\n",
    "\n",
    "# 2. Load Model\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import TextStreamer\n",
    "import gradio as gr\n",
    "\n",
    "print(\"‚¨áÔ∏è Loading Model from Hugging Face...\")\n",
    "\n",
    "# YOUR ADAPTER MODEL ON HUGGING FACE\n",
    "adapter_path = \"khairi123/medical-phi35-chatdoctor\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = adapter_path, # Loads the base model AND your adapter automatically\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"‚úÖ Model Loaded Successfully!\")\n",
    "\n",
    "# 3. Define the Chat Function\n",
    "def medical_chat(message, history):\n",
    "    prompt = f\"\"\"Below is an instruction that describes a medical task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{message}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True, skip_special_tokens = True)\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        streamer = streamer,\n",
    "        max_new_tokens = 512,\n",
    "        use_cache = True,\n",
    "        temperature = 0.1,\n",
    "        do_sample = True,\n",
    "    )\n",
    "\n",
    "    # We return an empty string here because the Streamer prints to console,\n",
    "    # but for Gradio we need to decode.\n",
    "    # Re-decoding for Gradio interface:\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return decoded.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# 4. Launch Gradio Interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=medical_chat,\n",
    "    title=\"üè• ChatDoctor - Medical AI Assistant\",\n",
    "    description=\"**Fine-tuned Phi-3.5 on 100k Medical Conversations.**\\n\\n‚ö†Ô∏è **Disclaimer:** This AI is for educational purposes only. Always consult a real doctor.\",\n",
    "    examples=[\"What are the symptoms of pneumonia?\", \"How to treat a fever at home?\", \"I have a sharp pain in my chest.\"],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "print(\"üöÄ Launching Chatbot...\")\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO/kcLq1FKuKdHHSVHf7La/",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
