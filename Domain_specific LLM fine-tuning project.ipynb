{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPuriPqcM9k8LJhie3aygLS"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueQxSD9HsXeo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764146803747,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "ba8b48e0-a37a-4c6b-cfb6-104850d69747"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0], sys.version_info(major=3, minor=12, micro=12, releaselevel='final', serial=0)\n",
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA: True\n",
      "CUDA version: 12.6\n",
      "GPU:Tesla T4\n",
      "Memory:15.828320GB\n",
      "\n",
      "âœ… T4 detected. Training will take longer.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python version: {sys.version}, {sys.version_info}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name=torch.cuda.get_device_name(0)\n",
    "    gpu_memory=torch.cuda.get_device_properties(0).total_memory /1e9\n",
    "    print(f\"GPU:{gpu_name}\")\n",
    "    print(f\"Memory:{gpu_memory:1f}GB\")\n",
    "\n",
    "\n",
    "    is_a100 = \"A100\" in gpu_name\n",
    "    print(f\"\\nâœ… {'A100 detected! Fast training mode.' if is_a100 else 'T4 detected. Training will take longer.'}\")\n",
    "else:\n",
    "    print(\"âŒ NO GPU! Change runtime: Runtime â†’ Change runtime type â†’ GPU\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pip install unsloth"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lvd2vxd5v7E2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764062822957,
     "user_tz": -60,
     "elapsed": 41329,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "f10c2123-bdb3-407a-807d-05c786519c50"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.11.3-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.11.4 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.11.4-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.24.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.5.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
      "Collecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.18.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\n",
      "Collecting trl!=0.19.0,<=0.23.0,>=0.18.2 (from unsloth)\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.22.1)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.11.4->unsloth)\n",
      "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.11.4->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.4->unsloth) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.11.4->unsloth)\n",
      "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n",
      "Downloading unsloth-2025.11.3-py3-none-any.whl (353 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m353.0/353.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.11.4-py3-none-any.whl (283 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m283.5/283.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
      "Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, shtab, pyarrow, msgspec, tyro, xformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "Successfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.20.0 pyarrow-22.0.0 shtab-1.8.0 torchao-0.14.1 trl-0.23.0 tyro-0.9.35 unsloth-2025.11.3 unsloth_zoo-2025.11.4 xformers-0.0.33.post1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q datasets\n",
    "!pip install -q gradio\n",
    "\n",
    "# Install Weights & Biases (Industry standard)\n",
    "!pip install -q wandb\n",
    "\n",
    "# Install evaluation tools\n",
    "!pip install -q nltk rouge-score\n",
    "\n",
    "print(\"âœ… All packages installed!\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7KIi0MyvihI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764055493443,
     "user_tz": -60,
     "elapsed": 44040,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "06c3f40d-cfaa-470c-c2bb-5925ba33deed"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "âœ… All packages installed!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Fix for numpy incompatibility with wandb\n",
    "# This cell will install numpy==1.26.4.\n",
    "# IMPORTANT: After running this cell, you must restart the runtime\n",
    "# (Runtime -> Restart runtime) for the changes to take effect before\n",
    "# re-running this cell or any subsequent cells that use wandb.\n",
    "!pip uninstall numpy -y\n",
    "!pip install numpy==1.26.4\n",
    "\n",
    "import wandb\n",
    "\n",
    "print(\"\\nğŸ“Š WEIGHTS & BIASES SETUP\")\n",
    "print(\"=\"*80)\n",
    "print(\"W&B is the industry standard for ML experiment tracking\")\n",
    "print(\"Used by: OpenAI, Hugging Face, Meta, Anthropic, Stability AI\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸ”‘ Get your API key:\")\n",
    "print(\" Go to: https://wandb.ai/authorize\")\n",
    "print(\" Copy your API key\")\n",
    "print(\" Paste below\\n\")\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "print(\"\\nâœ… W&B connected!\")\n",
    "print(\"ğŸ“Š View your experiments at: https://wandb.ai\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "1iWdg5zPw9bG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764146851768,
     "user_tz": -60,
     "elapsed": 37348,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "da1bd367-3ad8-49cc-a466-b686577eaa61"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: numpy 2.0.2\n",
      "Uninstalling numpy-2.0.2:\n",
      "  Successfully uninstalled numpy-2.0.2\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       },
       "id": "32d95084a8ff4378877f2a0ca261e35e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ“Š WEIGHTS & BIASES SETUP\n",
      "================================================================================\n",
      "W&B is the industry standard for ML experiment tracking\n",
      "Used by: OpenAI, Hugging Face, Meta, Anthropic, Stability AI\n",
      "================================================================================\n",
      "\n",
      "ğŸ”‘ Get your API key:\n",
      " Go to: https://wandb.ai/authorize\n",
      " Copy your API key\n",
      " Paste below\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhairiddineboukadida\u001b[0m (\u001b[33mkhairiddineboukadida-eniso\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… W&B connected!\n",
      "ğŸ“Š View your experiments at: https://wandb.ai\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"\\nğŸ“š Downloading ChatDoctor Medical Dataset\")\n",
    "print(\"=\"*80)\n",
    "print(\"Dataset: lavita/ChatDoctor-HealthCareMagic-100k\")\n",
    "print(\"Size: 100,000 medical Q&A pairs\")\n",
    "print(\"Source: Real patient-doctor conversations\")\n",
    "print(\"Format: Ready to use (no preprocessing needed!)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDownloading (~500MB, 2-3 minutes)...\\n\")\n",
    "\n",
    "dataset = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\")\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(15000))\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded!\")\n",
    "print(f\"Total samples: {len(train_dataset):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nğŸ“‹ Sample conversation:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nPatient: {sample['instruction'][:150]}...\")\n",
    "print(f\"\\nDoctor: {sample['output'][:150]}...\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478,
     "referenced_widgets": [
      "169455a98fa34c508147e51f9565c160",
      "683517e72fa946bc829b2f9ffb4033ce",
      "e53c605ca3ab4f4195512c4530d675b1",
      "aa5dd8c100474a49bd11b353b3561f1b",
      "0452eefec07740dcb0f57c6a21b0b3ab",
      "3d3a30682abf41188456c01efa91a8e6",
      "710aa29eb67b41279ee0b2036a1d0ade",
      "2a9b1012129b48bdbcc2be22c899d305",
      "c284969b2b8149b9bb963c345c5171be",
      "1101475678ec4e088a4fd770efa939e0",
      "9e8d933f6d204e959a5a6aa22b6a159f",
      "dd95bce2c6b64e96b80aefa8d7c6d6cb",
      "49e9c31b76e148249e2daf1c217d969b",
      "888a90f51a2b4387a4389265fff587ec",
      "68694dd63ae14d0aa4a724f308711cdf",
      "a7f383cc7c4c4edcb852d82ecf2e96de",
      "b33f5fb9aabe414bbdbf520e67c9f175",
      "0e8fdbf50f5149df9eb6cf48fb533ce6",
      "30eede4e01064025bd03183e36776e4c",
      "6973bbad826b41a484a84174e9b364d2",
      "dac4deafde514a69aeb3a255d47846ac",
      "114d5c1cf332442fa123bc57d10fd07b",
      "330a4790586f48dca92dcf192d2138c0",
      "54ddd00fa23346f6844b567d1787c319",
      "2e05231671ed4b8b8309729e7a6fc033",
      "d2a3c02718b6499383207e91bbb96b54",
      "4a153f5469704c20ba905a9cbf88e57a",
      "331c3ed29a114e3495500d5a85dbeb5d",
      "d41187f7e5014b42bf3363edb2d107b3",
      "6458fc745a0f4b998095ba5966bfcb12",
      "a92da5c7ca194621a4be7a02a34ceacf",
      "f5c494c95c604093a46670c42ccb8260",
      "8559365297944f65ad3d20e97b1e8d1a"
     ]
    },
    "id": "5P7FQw7G0KS-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764145465445,
     "user_tz": -60,
     "elapsed": 7270,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "43d82e31-ba25-4c2a-804c-454a435b188b"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ“š Downloading ChatDoctor Medical Dataset\n",
      "================================================================================\n",
      "Dataset: lavita/ChatDoctor-HealthCareMagic-100k\n",
      "Size: 100,000 medical Q&A pairs\n",
      "Source: Real patient-doctor conversations\n",
      "Format: Ready to use (no preprocessing needed!)\n",
      "================================================================================\n",
      "\n",
      "Downloading (~500MB, 2-3 minutes)...\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/542 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169455a98fa34c508147e51f9565c160"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001-5e7cb295b9cff0(â€¦):   0%|          | 0.00/70.5M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd95bce2c6b64e96b80aefa8d7c6d6cb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/112165 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "330a4790586f48dca92dcf192d2138c0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… Dataset loaded!\n",
      "Total samples: 15,000\n",
      "\n",
      "ğŸ“‹ Sample conversation:\n",
      "\n",
      "Patient: If you are a doctor, please answer the medical questions based on the patient's description....\n",
      "\n",
      "Doctor: Hi thanks for asking question. Here you are complaining pain in particular position esp. While turning to a side. So strong possibility is about moder...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "print(\"\\nğŸ”„ Creating train/validation/test splits...\")\n",
    "\n",
    "# Split: 90% train, 5% validation, 5% test\n",
    "train_test = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "val_test = train_test['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': val_test['train'],\n",
    "    'test': val_test['test']\n",
    "})\n",
    "\n",
    "print(f\"\\nâœ… Data splits created:\")\n",
    "print(f\"   Train:      {len(final_dataset['train']):>7,} samples (90%)\")\n",
    "print(f\"   Validation: {len(final_dataset['validation']):>7,} samples (5%)\")\n",
    "print(f\"   Test:       {len(final_dataset['test']):>7,} samples (5%)\")\n",
    "\n",
    "# Save to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "save_path = '/content/drive/MyDrive/medical_llm_data'\n",
    "final_dataset.save_to_disk(save_path)\n",
    "print(f\"\\nğŸ’¾ Dataset saved to: {save_path}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "e171b5401ac047b38bcece845d8494a5",
      "85e7d0f16dc94165be05e5d17ba75b23",
      "a92e1e64de5d41509dc0d154dfd8d3d8",
      "fd8e9a7325c04bf591a85711cd6134aa",
      "cbed5fefae0d45cb8158b368a96171f4",
      "c61f2c91ec05499dbda2bc941970eea2",
      "ee50c52223df40e9abd045dc44254d0b",
      "8bc5615233d7499bb899727fb0fd58a2",
      "8c823893133b40cd8e396f8fb45bb73e",
      "c3dd637473e94283a0f1e64d6b95eadb",
      "e94b1351976a4b5eb5be95eb43ff449d",
      "ce7d9a598aff419c94a2bd536aa3e024",
      "844c95d2f8894578bf6fb1cd2d90bea0",
      "feb7d56224b94f1299a6d39ac5ac82de",
      "b2067d9bf9ac455e8132c1df4464f8ff",
      "f3247444213f4870a98a6698ce590941",
      "5aedf5fe4ffd4f4fbaf7c5fa7b6b24f0",
      "98a4d5ec42a44bb8ba79bec1d9ad6c9e",
      "dee5201548fe482c8305b007b54cba1e",
      "a600cb9e34b646f984bb1c251a0a5ee6",
      "8ee6102b73bc42978a9dd0bb949e2a97",
      "353ccaa27c1f46129fd887147fe8310f",
      "a5aac0d1ca8c48cfba3d415315876f0e",
      "271b7beb1a4d47c5aa69ac220e378328",
      "2de5eed057374cf2b3b3bfed7aff4c5c",
      "3475e3a76e0c4908af6af5dad3be8a9f",
      "85db35a68135448eb3764aca7308695a",
      "e06f0a8a3dde4d0baf9c47bb8e64018e",
      "c0671b52916e461392ef450cb2326593",
      "a9627c7818964124a1040a84f814d86f",
      "ebfbfe9df19b4c31b553879712a47a90",
      "fcc7053aef424911b19f38d86c62e6d3",
      "aaa8b24c8d8f4d85aa5073282ef96d19"
     ]
    },
    "id": "oQWxDeU-4asW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764145476463,
     "user_tz": -60,
     "elapsed": 8272,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "86969f5a-b50c-43b8-c4ef-1eae320aa1fe"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ”„ Creating train/validation/test splits...\n",
      "\n",
      "âœ… Data splits created:\n",
      "   Train:       13,500 samples (90%)\n",
      "   Validation:     750 samples (5%)\n",
      "   Test:           750 samples (5%)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/13500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e171b5401ac047b38bcece845d8494a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/750 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce7d9a598aff419c94a2bd536aa3e024"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/750 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5aac0d1ca8c48cfba3d415315876f0e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ’¾ Dataset saved to: /content/drive/MyDrive/medical_llm_data\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    \"max_seq_length\": 2048,\n",
    "\n",
    "    # Training (optimized for GPU type)\n",
    "    \"num_epochs\": 1,\n",
    "    \"batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": False,\n",
    "\n",
    "    # LoRA\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0,\n",
    "\n",
    "    # Logging\n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "\n",
    "    # Output\n",
    "    \"output_dir\": \"./medical-llm-output\",\n",
    "\n",
    "    # W&B\n",
    "    \"wandb_project\": \"medical-llm-finetuning\",\n",
    "    \"wandb_run_name\": f\"phi3.5-chatdoctor-{gpu_name.replace(' ', '-').lower()}\",\n",
    "    \"wandb_tags\": [\"medical\", \"phi-3.5\", \"qlora\", \"chatdoctor\", \"unsloth\"],\n",
    "}\n",
    "\n",
    "print(\"\\nâš™ï¸ TRAINING CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: Phi-3.5-mini-instruct (4B)\")\n",
    "print(f\"Dataset: ChatDoctor (100K samples)\")\n",
    "print(f\"Method: QLoRA with Unsloth optimization\")\n",
    "print(f\"\\nGPU: {gpu_name}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Gradient accumulation: {CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"\\nEstimated training time: {4 if is_a100 else 8} hours\")\n",
    "print(f\"Cost (Colab Pro): ${(4 if is_a100 else 8) * 1.67:.2f}\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXo6FpXJb4hG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764146857333,
     "user_tz": -60,
     "elapsed": 33,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "fdf76490-c7cf-4c1b-a01f-65b52a29c08d"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âš™ï¸ TRAINING CONFIGURATION\n",
      "================================================================================\n",
      "Model: Phi-3.5-mini-instruct (4B)\n",
      "Dataset: ChatDoctor (100K samples)\n",
      "Method: QLoRA with Unsloth optimization\n",
      "\n",
      "GPU: Tesla T4\n",
      "Batch size: 8\n",
      "Gradient accumulation: 2\n",
      "Effective batch size: 16\n",
      "\n",
      "Estimated training time: 8 hours\n",
      "Cost (Colab Pro): $13.36\n",
      "================================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"\\nğŸ¤– LOADING MODEL WITH UNSLOTH\")\n",
    "print(\"=\"*80)\n",
    "print(\"Downloading  Phi-3.5-mini-instruct (4B)\")\n",
    "print(\"This takes 5-10 minutes on first run...\")\n",
    "print(\"Subsequent runs use cached model\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    dtype=None,  # Auto-detect best dtype\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    ")\n",
    "\n",
    "print(\"âœ… Base model loaded!\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e9:.2f}B\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "print(\"\\nğŸ”§ Adding LoRA adapters...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š TRAINABLE PARAMETERS:\")\n",
    "model.print_trainable_parameters()\n",
    "print(\"\\nâœ… Model ready for training!\")"
   ],
   "metadata": {
    "id": "G43LocRyEqnX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\nğŸ”„ Formatting dataset for training...\")\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = \"\"\"Below is an instruction that describes a medical task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_func(examples):\n",
    "    \"\"\"Format examples for Unsloth\"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = prompt_template.format(instruction, output)\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Format datasets\n",
    "train_dataset = final_dataset[\"train\"].map(\n",
    "    formatting_func,\n",
    "    batched=True,\n",
    "    remove_columns=final_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "eval_dataset = final_dataset[\"validation\"].map(\n",
    "    formatting_func,\n",
    "    batched=True,\n",
    "    remove_columns=final_dataset[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "print(f\"âœ… Datasets formatted!\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Validation: {len(eval_dataset):,} samples\")\n"
   ],
   "metadata": {
    "id": "5jPIZOXZFYIw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "print(\"\\nğŸ“‹ Setting up trainer...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    eval_strategy=\"steps\", # Changed from evaluation_strategy\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    # W&B integration\n",
    "    report_to=\"wandb\",\n",
    "    run_name=CONFIG[\"wandb_run_name\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    args=training_args,\n",
    "    packing=False,  # Can enable for 30% speed boost\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer configured!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133,
     "referenced_widgets": [
      "3c972c1f2c4e410294854bf517210196",
      "836216309a2a47b69c1373161b076292",
      "9c9932b238994792bfd572e7a9090248",
      "2cbfbe60fdc14d188a4e088db111352e",
      "54284d35f0f8411180ce34b4d7a30b71",
      "8dccd38a83ea4338a4c2ecbde15a5c42",
      "aa95601718e147ebac44c377a3d6b47d",
      "34f2101a80b745aab36ca02ed96b40c6",
      "184aa722d5aa4da988e5663a1cf93e6b",
      "02b400b443fa43f0b696443511ecea9f",
      "0fac119ae8974130a86156c23eb3ac42",
      "1a849066ec0b4a0a86f413463e34ecd4",
      "27e97d24520f4994833307a34081829e",
      "43bd8e91edc54f42a231cac6fff6a8bd",
      "c00e00b6f8e040e9a4b87811b825f6cd",
      "abf7d87b231d498aa055a5292d176564",
      "8b9bc990bc9948b6bfb007ff026ea5b8",
      "c66106ed285443f9beae25cc2c564d05",
      "ee2841bc945942b3a43bbfb4c119ec57",
      "7ba579ffb82e495f8502ee37a104144d",
      "ccff03d7133142fd932180c1f28c7743",
      "0e72dbc431cb4161a836c5933bbf9318"
     ]
    },
    "id": "mig1nY2OFv0P",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764055830832,
     "user_tz": -60,
     "elapsed": 21247,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "7c516067-cbe8-4099-cf4c-a430028acf41"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ“‹ Setting up trainer...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/13500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c972c1f2c4e410294854bf517210196"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/750 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a849066ec0b4a0a86f413463e34ecd4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Trainer configured!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‹ï¸ STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸ“Š W&B Project: {CONFIG['wandb_project']}\")\n",
    "print(f\"ğŸ·ï¸  Run Name: {CONFIG['wandb_run_name']}\")\n",
    "print(f\"â±ï¸  Expected Duration: {6 if is_a100 else 10} hours\")\n",
    "print(f\"ğŸ“ˆ Monitor progress at: https://wandb.ai\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸš€ Training started! Go grab coffee â˜•\\n\")\n",
    "print(\"ğŸ’¡ TIP: Keep this tab open or it will disconnect\")\n",
    "print(\"ğŸ’¡ TIP: Checkpoints auto-save every 500 steps\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Initialize W&B run with config\n",
    "wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    name=CONFIG[\"wandb_run_name\"],\n",
    "    config=CONFIG,\n",
    "    tags=CONFIG[\"wandb_tags\"],\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Training complete\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"â±ï¸  Total time: {trainer_stats.metrics['train_runtime'] / 3600:.2f} hours\")\n",
    "print(f\"ğŸ“Š Samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"ğŸ”¥ Speedup vs standard: ~2-3x faster with Unsloth!\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6CQisDFeJZgQ",
    "outputId": "5d2cafe2-ba80-4879-9e39-7af365d9ad2d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764062010556,
     "user_tz": -60,
     "elapsed": 6173866,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ‹ï¸ STARTING TRAINING\n",
      "================================================================================\n",
      "ğŸ“Š W&B Project: medical-llm-finetuning\n",
      "ğŸ·ï¸  Run Name: phi3.5-chatdoctor-tesla-t4\n",
      "â±ï¸  Expected Duration: 10 hours\n",
      "ğŸ“ˆ Monitor progress at: https://wandb.ai\n",
      "================================================================================\n",
      "\n",
      "ğŸš€ Training started! Go grab coffee â˜•\n",
      "\n",
      "ğŸ’¡ TIP: Keep this tab open or it will disconnect\n",
      "ğŸ’¡ TIP: Checkpoints auto-save every 500 steps\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251125_073039-otvt4jjy</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khairiddineboukadida-eniso/medical-llm-finetuning/runs/otvt4jjy' target=\"_blank\">phi3.5-chatdoctor-tesla-t4</a></strong> to <a href='https://wandb.ai/khairiddineboukadida-eniso/medical-llm-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/khairiddineboukadida-eniso/medical-llm-finetuning' target=\"_blank\">https://wandb.ai/khairiddineboukadida-eniso/medical-llm-finetuning</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/khairiddineboukadida-eniso/medical-llm-finetuning/runs/otvt4jjy' target=\"_blank\">https://wandb.ai/khairiddineboukadida-eniso/medical-llm-finetuning/runs/otvt4jjy</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 13,500 | Num Epochs = 1 | Total steps = 844\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 29,884,416 of 3,850,963,968 (0.78% trained)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='844' max='844' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [844/844 1:41:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.625700</td>\n",
       "      <td>1.690497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.550700</td>\n",
       "      <td>1.601047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.546400</td>\n",
       "      <td>1.561975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.517100</td>\n",
       "      <td>1.534335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.513200</td>\n",
       "      <td>1.509759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.472900</td>\n",
       "      <td>1.492677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.449200</td>\n",
       "      <td>1.479799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.402700</td>\n",
       "      <td>1.473471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "================================================================================\n",
      "â±ï¸  Total time: 1.71 hours\n",
      "ğŸ“Š Samples/second: 2.19\n",
      "ğŸ”¥ Speedup vs standard: ~2-3x faster with Unsloth!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"ğŸ’¾ Saving model...\")\n",
    "\n",
    "# Save locally\n",
    "output_path = CONFIG[\"output_dir\"] + \"/final\"\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "print(f\"âœ… Saved locally: {output_path}\")\n",
    "\n",
    "# Save to Google Drive\n",
    "drive_path = '/content/drive/MyDrive/medical_llm_model'\n",
    "model.save_pretrained(drive_path)\n",
    "tokenizer.save_pretrained(drive_path)\n",
    "print(f\"âœ… Saved to Drive: {drive_path}\")\n",
    "\n",
    "# Log model to W&B\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"medical-llm-model\",\n",
    "    type=\"model\",\n",
    "    description=\"Fine-tuned Llama 3.1 8B on ChatDoctor medical dataset\"\n",
    ")\n",
    "artifact.add_dir(output_path)\n",
    "wandb.log_artifact(artifact)\n",
    "print(\"âœ… Logged to W&B Artifacts\")"
   ],
   "metadata": {
    "id": "fcMFD3e7Je7-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764062050889,
     "user_tz": -60,
     "elapsed": 12375,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "2e748c8b-eef4-47bc-d052-af0a372f4938"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ’¾ Saving model...\n",
      "âœ… Saved locally: ./medical-llm-output/final\n",
      "âœ… Saved to Drive: /content/drive/MyDrive/medical_llm_model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (medical-llm-output/final)... Done. 4.9s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Logged to W&B Artifacts\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "yhRWJylk9w1e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764144057273,
     "user_tz": -60,
     "elapsed": 19208,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "db8c8b06-904b-448e-fc5e-af8216d9e59a"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q gradio\n",
    "from google.colab import drive\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"ğŸ“ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# âœ… CORRECT PATH - This is where your model is saved!\n",
    "MODEL_PATH = \"/content/drive/MyDrive/medical_llm_model\"\n",
    "\n",
    "print(f\"\\nğŸ”„ Loading model from: {MODEL_PATH}\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Load model with LoRA adapters\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_PATH,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Switch to inference mode (important!)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\\n\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647,
     "referenced_widgets": [
      "db057874754e4f7092ff35fe2e83a68d",
      "0d414e2039cb458f87a12aba851ead67",
      "f00ae1f9e43b49efb619627a2dbef71d",
      "59142a25cd46462fb53d1dac02a6a6c7",
      "5381f04f49d748fa9223ec0286b7741f",
      "c59190be562e42cb80dcc6f777589175",
      "8f9e06ef0abc4a8594047fb5577eaf2e",
      "29f6e861290447b28ec16d518031e2d7",
      "1af4fb69d9834d8fb8f06c74b0ea1cd7",
      "406edf4f16954feca11f3035188e534a",
      "f4aa18e102184cb0b0944c8ff5f37271",
      "58587ceb2ee4424ba26996faca195dae",
      "24d49a4d934b40d7aab76a8c19d9199d",
      "e356db39ab6643839fd6166d6e7535f6",
      "64c73109a93249e3b7354703327074be",
      "ea773eef0bb347d19d5a7a5628941402",
      "c51be0b5e1eb4795a2dd6dcaea24e397",
      "5c7a2ea15b404befafd96ce908681445",
      "75a86266b18f409da4f299b3c94054ef",
      "4709db558769441eaedfeceae75a0b13",
      "33a778074d2c46b8b0c340cbd381acb1",
      "1a7af3f6ca974ea6a19b15cd762de2d1"
     ]
    },
    "id": "U52ZPkCAKUxl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764144244451,
     "user_tz": -60,
     "elapsed": 113166,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "571af4d4-bad5-47b2-fa21-2202282cfa3c"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.4/284.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "ğŸ“ Mounting Google Drive...\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\n",
      "ğŸ”„ Loading model from: /content/drive/MyDrive/medical_llm_model\n",
      "This may take 2-3 minutes...\n",
      "\n",
      "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db057874754e4f7092ff35fe2e83a68d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/140 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58587ceb2ee4424ba26996faca195dae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth 2025.11.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Model loaded successfully!\n",
      "GPU: Tesla T4\n",
      "Memory: 15.8GB\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"ğŸ§ª Testing model with sample questions...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prompt template (same as training)\n",
    "prompt_template = \"\"\"Below is an instruction that describes a medical task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def ask_question(question, max_tokens=512, temperature=0.7):\n",
    "    \"\"\"Ask your trained model a question\"\"\"\n",
    "    prompt = prompt_template.format(question)\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    answer = response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Test with sample questions\n",
    "test_questions = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is high blood pressure treated?\",\n",
    "    \"What causes migraine headaches?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[Test {i}] Question: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    answer = ask_question(question)\n",
    "\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Model is working! Ready for Gradio interface.\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFVw2aomLpxF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764144420216,
     "user_tz": -60,
     "elapsed": 170466,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "4c38bc18-db0b-478f-bc50-9c8c63bccc1e"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ§ª Testing model with sample questions...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Test 1] Question: What are the symptoms of diabetes?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: High blood sugar levels can cause many symptoms, such as increased thirst, frequent urination, tiredness, blurred vision, and slow-healing cuts and sores. Some people may not have symptoms. This is why it is important to get your blood sugar checked regularly.  You should also know that diabetes can cause problems with your eyes, kidnes, nerves, and circulatory system.  If you have diabetes, you should see your doctor regularly to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  You should also see your doctor regularly to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled.  If you have diabetes, you should also have regular checkups to make sure that your diabetes is well controlled\n",
      "================================================================================\n",
      "\n",
      "[Test 2] Question: How is high blood pressure treated?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Lifestyle changes are important to prevent high blood pressure. A healthy diet, regular exercise, and weight loss are all important. If these changes are not enough, medications are used to help lower blood pressure.  A number of Chat Doctor.  Common medications used to lower blood pressure include ACE inhibitors (lisinopril, enalapril), angiotensin II receptor blockers (losartan, valsartan), calcium channel blockers (amlodipine, diltiazem, verapamil), and thiazide diuretics (chlorthalidone, hydrochlorothiazide).  If a patient is not responding to these medications, a different class of blood pressure medication may be used.  These medications are safe and effective when used appropriately.  They are also available in a variety of doses, allowing for a dose to be chosen that will work well for each individual.  A follow-sit with your physician is necessary to determine the proper dose and medication to use.  Blood pressure medications are safe, and there are no known interactions between these medications and other commonly used medications.  In addition, these medications have been shown to be safe during pregnancy.  Most patients with high blood pressure are able to be treated with lifestyle changes alone.  However, some patients will require medication to control their blood pressure.  It is important to know that blood pressure medications are very safe, and there are no known interactions between blood pressure medications and other commonly used medications.  Therefore, blood pressure medications are safe to use during pregnancy.  In addition, it is important to know that blood pressure medications are very safe, and there are no known interactions between blood pressure medications and other commonly used medications.  Therefore, blood pressure medications are safe to use during pregnancy.  In addition, it is important to know that blood pressure medications are very safe, and there are no known interactions between blood pressure medications and other commonly used medications.  Therefore, blood pressure medications are safe to use during pregnancy.  In addition, it is important to know that blood pressure medications are very safe, and there are no known interactions between blood pressure medications and other commonly used medications.  Therefore, blood pressure medications are safe to use during pre\n",
      "================================================================================\n",
      "\n",
      "[Test 3] Question: What causes migraine headaches?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Migraine is a type of headache that causes severe throbbing pain on one side of your head. Migraine attacks can last for hours to days. The throbbing pain can be so severe that you can't do your usual activities. Most people who get migraines are younger than 50, and women get them more often than men. The cause of migraine is not known. Most doctors believe that genetics plays a role in migraines. If you have a close relative who gets migraines, you are more likely to get them. Other factors that can trigger migraines include stress, hormonal changes, certain foods, and some medications. Migraines are usually treated with pain relievers. If the pain is severe, your doctor may prescribe medications to relieve the nausea, vomiting, and sensitivity to light and sound that often accompany migraines. For more information on migraines, please visit http://www.WebMD.com/health/migraine-headache-in-children.  In my opinion, the cause of migraines is genetics and stress. I hope this information is helpful to you. Please feel free to ask any further questions. Wish you good health. Regards.  Chat Doctor. MD. Rana. Genl. Prabhakar. Lecturer, Advisor. C.P.T., A.A.C., I.C.M., C.D.S. Genl. Prabhakar can be reached at the following URL http://www.ChatDoctor.com/doctors/genl-prabhakar.html. Thank you. Have a nice day. Chat Doctor. MD. Rana. Genl. Prabhakar. Lecturer, Advisor. C.P.T., A.A.C., I.C.M., C.D.S. http://www.ChatDoctor.com/doctors/genl-prabhakar.html. Thank you. Have a nice day. Chat Doctor. MD. Rana. Genl. Prabhakar. Lecturer, Advisor. C.P.T., A.A.C., I.C.M., C.D.S. http://www.ChatDoctor.\n",
      "================================================================================\n",
      "\n",
      "âœ… Model is working! Ready for Gradio interface.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def medical_chat(question, max_length, temperature):\n",
    "    \"\"\"Chat function with customizable parameters\"\"\"\n",
    "\n",
    "    if not question.strip():\n",
    "        return \"âš ï¸ Please enter a medical question.\", \"N/A\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    answer = ask_question(question, max_tokens=int(max_length), temperature=temperature)\n",
    "\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    return answer, f\"â±ï¸ Response time: {latency:.2f}s\"\n",
    "\n",
    "# Create beautiful Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=medical_chat,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            label=\"ğŸ’¬ Ask a Medical Question\",\n",
    "            placeholder=\"Example: What are the symptoms of type 2 diabetes?\",\n",
    "            lines=3\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=128,\n",
    "            maximum=1024,\n",
    "            value=512,\n",
    "            step=64,\n",
    "            label=\"Max Response Length\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.7,\n",
    "            step=0.1,\n",
    "            label=\"Temperature (Lower = More Factual)\"\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(\n",
    "            label=\"ğŸ¤– AI Response\",\n",
    "            lines=10,\n",
    "            show_copy_button=True\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Performance\",\n",
    "            lines=1\n",
    "        ),\n",
    "    ],\n",
    "    title=\"ğŸ¥ Medical AI Assistant - Phi-3.5-mini Fine-tuned\",\n",
    "    description=\"\"\"\n",
    "    **Fine-tuned on 100,000 medical conversations**\n",
    "\n",
    "    Ask any medical question and get an AI-powered response based on the ChatDoctor dataset.\n",
    "\n",
    "    âš ï¸ **IMPORTANT:** This is for educational purposes only. Always consult a real healthcare professional.\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        [\"What are the symptoms of diabetes?\", 512, 0.7],\n",
    "        [\"How is high blood pressure treated?\", 512, 0.7],\n",
    "        [\"What causes migraine headaches?\", 512, 0.7],\n",
    "        [\"Explain what pneumonia is.\", 512, 0.7],\n",
    "        [\"What are the risk factors for heart disease?\", 512, 0.7],\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    "    article=\"\"\"\n",
    "    ---\n",
    "    ### About This Model\n",
    "    - **Base:** Microsoft Phi-3.5-mini-instruct (4B parameters)\n",
    "    - **Training:** Fine-tuned on ChatDoctor-100k with QLoRA\n",
    "    - **Speed:** Optimized with Unsloth (2-5x faster)\n",
    "    - **Hardware:** Trained on Tesla T4 GPU\n",
    "\n",
    "    ### Disclaimer\n",
    "    This AI assistant is for educational purposes only and should not be used as a substitute\n",
    "    for professional medical advice, diagnosis, or treatment.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Launch with public link\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ LAUNCHING GRADIO INTERFACE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸ“Š Your fine-tuned model is ready to use!\")\n",
    "print(\"ğŸŒ Creating public link for sharing...\\n\")\n",
    "\n",
    "demo.launch(\n",
    "    share=True,  # Creates public shareable link\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Gradio interface launched!\")\n",
    "print(\"ğŸŒ Public link created - you can share this link!\")\n",
    "print(\"ğŸ“± Perfect for LinkedIn/Portfolio demonstrations\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "id": "3SiY1zwJLxSl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764144756189,
     "user_tz": -60,
     "elapsed": 330462,
     "user": {
      "displayName": "Khairiddine Boukadida",
      "userId": "17357550376728438007"
     }
    },
    "outputId": "cb9cd78a-1047-47b9-eb5f-9838464516a1"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ LAUNCHING GRADIO INTERFACE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Your fine-tuned model is ready to use!\n",
      "ğŸŒ Creating public link for sharing...\n",
      "\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://88204218917e2b637e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://88204218917e2b637e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://88204218917e2b637e.gradio.live\n",
      "\n",
      "âœ… Gradio interface launched!\n",
      "ğŸŒ Public link created - you can share this link!\n",
      "ğŸ“± Perfect for LinkedIn/Portfolio demonstrations\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xP5MZjcRQH2t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Bon8eN3kJum9"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}